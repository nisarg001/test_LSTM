{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_StockPrediction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisarg001/test_LSTM/blob/master/LSTM_StockPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41up2YhJf7Iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "df = pd.read_csv('Nifty 50 Historical Data.csv',delimiter=',',usecols=['Date','Open','High','Low','Close', 'Volume'])\n",
        "\n",
        "#values = df.values()\n",
        "\n",
        "# Sort DataFrame by date\n",
        "#df = df.sort_values('Date')\n",
        "\n",
        "# Double check the result\n",
        "df.head()\n",
        "\n",
        "plt.figure(figsize = (18,9))\n",
        "plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)\n",
        "plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\n",
        "plt.xlabel('Date',fontsize=18)\n",
        "plt.ylabel('Mid Price',fontsize=18)\n",
        "plt.show()\n",
        "\n",
        "df['mid'] = (df['Low']+df['High'])/2.0\n",
        "\n",
        "SEQ_LEN = 100  # how long of a preceeding sequence to collect for RNN\n",
        "FUTURE_PERIOD_PREDICT = 1  # how far into the future are we trying to predict?\n",
        "RATIO_TO_PREDICT = \"Close\"\n",
        "\n",
        "def classify(current, future):\n",
        "    if float(future) > float(current):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "      \n",
        "\n",
        "df['future'] = df[RATIO_TO_PREDICT].shift(-FUTURE_PERIOD_PREDICT)\n",
        "df['target'] = list(map(classify, df[RATIO_TO_PREDICT], df['future']))\n",
        "df.tail()\n",
        "\n",
        "times = sorted(df.index.values)  # get the times\n",
        "last_10pct = sorted(df.index.values)[-int(0.05*len(times))]  # get the last 10% of the times\n",
        "last_20pct = sorted(df.index.values)[-int(0.1*len(times))]  # get the last 20% of the times\n",
        "\n",
        "test_df = df[(df.index >= last_10pct)]\n",
        "validation_df = df[(df.index >= last_20pct) & (df.index < last_10pct)]  \n",
        "train_df = df[(df.index < last_20pct)]  # now the train_df is all the data up to the last 20%\n",
        "\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "train_df.drop(columns=[\"Date\", \"future\", 'Open', 'High', 'Low', 'mid','target'], inplace=True)\n",
        "validation_df.drop(columns=[\"Date\", \"future\", 'Open', 'High', 'Low', 'mid','target'], inplace=True)\n",
        "test_df.drop(columns=[\"Date\", \"future\", 'Open', 'High', 'Low', 'mid','target'], inplace=True)# don't need this anymore.\n",
        "target_df = df['target'];\n",
        "target_test_df = target_df[(target_df.index >= last_10pct)]\n",
        "target_validation_df = target_df[(target_df.index >= last_20pct) & (target_df.index < last_10pct)]  \n",
        "target_train_df = target_df[(target_df.index < last_20pct)]  # now the train_df is all the data up to the last 20%\n",
        "\n",
        "\n",
        "print(train_df.head())\n",
        "#train_data = train_df[RATIO_TO_PREDICT].as_matrix()\n",
        "#valid_data = validation_df[RATIO_TO_PREDICT].as_matrix()\n",
        "#test_data = test_df[RATIO_TO_PREDICT].as_matrix()\n",
        "\n",
        "train_data = train_df.values\n",
        "valid_data = validation_df.values\n",
        "test_data = test_df.values\n",
        "target_train_data = target_train_df.values\n",
        "target_valid_data = target_validation_df.values\n",
        "target_test_data = target_test_df.values\n",
        "\n",
        "\n",
        "\n",
        "#train_data = train_data.reshape(-1,1)\n",
        "#valid_data = valid_data.reshape(-1,1)\n",
        "#test_data = test_data.reshape(-1,1)\n",
        "print(train_data.shape)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Train the Scaler with training data and smooth data\n",
        "smoothing_window_size = 1000\n",
        "for di in range(0,4000,smoothing_window_size):\n",
        "    print(di, smoothing_window_size)\n",
        "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
        "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
        "\n",
        "# You normalize the last bit of remaining data\n",
        "scaler.fit(train_data[di+smoothing_window_size:,:])\n",
        "train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])\n",
        "\n",
        "# Reshape both train and test data\n",
        "#train_data = train_data.reshape(-1)\n",
        "#print(train_data)\n",
        "\n",
        "# Normalize test data and validation data\n",
        "scaler.fit(valid_data)\n",
        "valid_data = scaler.transform(valid_data)\n",
        "scaler.fit(test_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "print(test_data)\n",
        "all_mid_data = np.concatenate([train_data,valid_data, test_data],axis=0)\n",
        "X_train = []\n",
        "y_train = []\n",
        "for i in range(SEQ_LEN, len(train_data) - FUTURE_PERIOD_PREDICT):\n",
        "    X_train.append(train_data[i-SEQ_LEN:i,:])\n",
        "    y_train.append(target_train_data[i-1 + (FUTURE_PERIOD_PREDICT-1)])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "X_valid = []\n",
        "y_valid = []\n",
        "for i in range(SEQ_LEN, len(valid_data)- FUTURE_PERIOD_PREDICT):\n",
        "    X_valid.append(valid_data[i-SEQ_LEN:i,:])\n",
        "    y_valid.append(target_valid_data[i-1+(FUTURE_PERIOD_PREDICT-1)])\n",
        "X_valid, y_valid = np.array(X_valid), np.array(y_valid)\n",
        "\n",
        "X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], X_valid.shape[2]))\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "for i in range(SEQ_LEN, len(test_data)- FUTURE_PERIOD_PREDICT):\n",
        "    X_test.append(test_data[i-SEQ_LEN:i,:])\n",
        "    y_test.append(target_test_data[i-1+(FUTURE_PERIOD_PREDICT-1)])\n",
        "    \n",
        "X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
        "\n",
        "X_train_2 = []\n",
        "y_train_2 = []\n",
        "for i in range(SEQ_LEN, len(train_data)- FUTURE_PERIOD_PREDICT):\n",
        "    X_train_2.append(train_data[i-SEQ_LEN:i])\n",
        "    y_train_2.append(target_train_data[i-1 + (FUTURE_PERIOD_PREDICT-1)])\n",
        "X_train_2, y_train_2 = np.array(X_train_2), np.array(y_train_2)\n",
        "\n",
        "X_train_2 = np.reshape(X_train_2, (X_train_2.shape[0], X_train_2.shape[1], X_train_2.shape[2]))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.plot(np.arange(y_train_2.shape[0]), y_train_2, color='blue', label='train target')\n",
        "\n",
        "plt.plot(np.arange(y_train_2.shape[0], y_train_2.shape[0]+y_valid.shape[0]), y_valid,\n",
        "         color='gray', label='valid target')\n",
        "\n",
        "plt.plot(np.arange(y_train_2.shape[0]+y_valid.shape[0],\n",
        "                   y_train_2.shape[0]+y_valid.shape[0]+y_test.shape[0]),\n",
        "         y_test, color='black', label='test target')\n",
        "\n",
        "\n",
        "plt.title('Some plot')\n",
        "plt.xlabel('time [days]')\n",
        "plt.ylabel('normalized price')\n",
        "plt.legend(loc='best');\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "X_train, y_train = shuffle(X_train, y_train)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvlVmCQ4Kj6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "import time\n",
        "\n",
        "#LSTM Try\n",
        "#EPOCHS = 10  # how many passes through our data\n",
        "BATCH_SIZE = 1024  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
        "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"  # a unique name for the model\n",
        "EPOCHS_LSTM = 100\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(SEQ_LEN, 2)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "# fit model\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS_LSTM,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "\n",
        "predicted_stock_price_multi_head = model.predict(X_test, verbose=0)\n",
        "#predicted_stock_price_multi_head = np.vstack((np.full((60,1), np.nan), predicted_stock_price_multi_head))\n",
        "plt.figure(figsize = (18,9))\n",
        "plt.plot(y_test, color = 'black', label = 'GE Stock Price')\n",
        "plt.plot(predicted_stock_price_multi_head, color = 'green', label = 'Predicted GE Mid Price')\n",
        "plt.title('GE Mid Price Prediction', fontsize=30)\n",
        "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('GE Mid Price')\n",
        "plt.legend(fontsize=18)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHaZZDJEuj2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pred_inv = scaler.inverse_transform(predicted_stock_price_multi_head[:,0])\n",
        "#print(pred_inv.shape)\n",
        "print(predicted_stock_price_multi_head.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "print(y_test)\n",
        "\n",
        "\n",
        "#aa = np.zeros((y_test.shape[0],1))\n",
        "#y_test2 = np.append(y_test.reshape(y_test.shape[0],1), aa, axis = 1)\n",
        "#y_test_inv = scaler.inverse_transform(y_test2)\n",
        "#print(y_test_inv[:, 0])\n",
        "\n",
        "#predicted_stock_price_multi_head2 = np.append(predicted_stock_price_multi_head, aa, axis=1)\n",
        "#pred_inv = scaler.inverse_transform(predicted_stock_price_multi_head2)\n",
        "#print(prev_inv[:, 0])\n",
        "#y_test_inv = scaler.inverse_transform(y_test)\n",
        "#print(y_test_inv)\n",
        "#print(y_test_inv.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGeR7mmDB_hV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}